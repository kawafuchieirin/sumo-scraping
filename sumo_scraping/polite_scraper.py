#!/usr/bin/env python3
import asyncio
from playwright.async_api import async_playwright
import logging
import json
import os
import argparse
from datetime import datetime
import pandas as pd
from pydantic import ValidationError
import random

try:
    from .models import PropertyInfo, RoomInfo, ScrapingResult
    from .station_mapping import get_station_url, get_supported_stations, is_yamanote_station, get_yamanote_stations
    from .rate_limiter import PoliteRequestManager, RateLimitConfig
except ImportError:
    from models import PropertyInfo, RoomInfo, ScrapingResult
    from station_mapping import get_station_url, get_supported_stations, is_yamanote_station, get_yamanote_stations
    from rate_limiter import PoliteRequestManager, RateLimitConfig

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('polite_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PoliteSuumoScraper:
    """ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’é…æ…®ã—ãŸç¤¼å„€æ­£ã—ã„SUUMOã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, stations=["æ¸‹è°·"], target_count=100, prefecture="tokyo", polite_mode=True):
        self.stations = stations if isinstance(stations, list) else [stations]
        self.target_count = target_count
        self.prefecture = prefecture
        self.base_url = "https://suumo.jp"
        self.properties = []
        self.validation_errors = []
        self.current_station = None
        
        # ç¤¼å„€æ­£ã—ã„ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
        if polite_mode:
            config = RateLimitConfig(
                min_delay=5.0,      # ã‚ˆã‚Šé•·ã„å¾…æ©Ÿæ™‚é–“
                max_delay=12.0,
                page_delay=8.0,     # ãƒšãƒ¼ã‚¸é–“å¾…æ©Ÿã‚’é•·ã
                station_delay=15.0, # é§…é–“å¾…æ©Ÿã‚’é•·ã
                max_retries=3,
                retry_delay=10.0,   # ãƒªãƒˆãƒ©ã‚¤é–“éš”ã‚’é•·ã
                concurrent_limit=1  # åŒæ™‚å®Ÿè¡Œã‚’1ã¤ã«åˆ¶é™
            )
        else:
            config = RateLimitConfig()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        
        self.request_manager = PoliteRequestManager(config)
        
    async def create_browser(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox', 
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',  # è‡ªå‹•åŒ–æ¤œå‡ºå›é¿
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',  # ç”»åƒèª­ã¿è¾¼ã¿ç„¡åŠ¹åŒ–ã§è² è·è»½æ¸›
                '--disable-javascript',  # å¿…è¦æœ€å°é™ã®JSä»¥å¤–ç„¡åŠ¹åŒ–
            ]
        )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆæ™‚ã«è¨­å®š
        context = await self.browser.new_context(
            user_agent=self.request_manager.get_current_user_agent(),
            viewport={'width': 1366, 'height': 768},  # ä¸€èˆ¬çš„ãªè§£åƒåº¦
            locale='ja-JP',
            timezone_id='Asia/Tokyo'
        )
        
        self.page = await context.new_page()
        
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š
        self.page.set_default_navigation_timeout(60000)
        self.page.set_default_timeout(30000)
        
    async def close_browser(self):
        if hasattr(self, 'page'):
            await self.page.close()
        if hasattr(self, 'browser'):
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def extract_property_info(self, property_element):
        """ç‰©ä»¶æƒ…å ±ã®æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""
        try:
            property_data = {}
            
            # ã‚ˆã‚Šå …ç‰¢ãªã‚»ãƒ¬ã‚¯ã‚¿æ¤œç´¢
            async def safe_extract_text(selectors, default=""):
                for selector in selectors:
                    try:
                        elem = await property_element.query_selector(selector)
                        if elem:
                            text = await elem.inner_text()
                            if text and text.strip():
                                return text.strip()
                    except Exception as e:
                        logger.debug(f"Selector failed {selector}: {e}")
                        continue
                return default
            
            # ç‰©ä»¶å
            title_selectors = [
                'h2.cassetteitem_content-title',
                'h3.cassetteitem_content-title', 
                'div.cassetteitem_content-title',
                '.cassetteitem_content-title',
                'h2[class*="title"]',
                'h3[class*="title"]',
                'div[class*="title"]'
            ]
            property_data['title'] = await safe_extract_text(title_selectors)
            
            # ä½æ‰€
            address_selectors = [
                'li.cassetteitem_detail-col1',
                'div.cassetteitem_detail-col1',
                '.cassetteitem_detail-col1'
            ]
            property_data['address'] = await safe_extract_text(address_selectors)
            
            # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±
            access_selectors = [
                'li.cassetteitem_detail-col2',
                'div.cassetteitem_detail-col2',
                '.cassetteitem_detail-col2'
            ]
            property_data['access'] = await safe_extract_text(access_selectors)
            
            # ç¯‰å¹´æ•°ãƒ»é¢ç©æƒ…å ±
            age_area_selectors = [
                'li.cassetteitem_detail-col3',
                'div.cassetteitem_detail-col3',
                '.cassetteitem_detail-col3'
            ]
            property_data['building_age_area'] = await safe_extract_text(age_area_selectors)
            
            # éƒ¨å±‹æƒ…å ±ã®æŠ½å‡ºï¼ˆã‚ˆã‚Šå®‰å…¨ã«ï¼‰
            rooms_info = []
            try:
                room_rows = await property_element.query_selector_all('tbody tr.js-cassette_link')
                
                for row in room_rows:
                    room_data = {}
                    
                    # å„éƒ¨å±‹ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«æŠ½å‡º
                    room_data['floor'] = await safe_extract_text(['td.ui-text--midium'], default="")
                    room_data['rent'] = await safe_extract_text(['span.cassetteitem_price--rent'], default="")
                    room_data['admin_fee'] = await safe_extract_text(['span.cassetteitem_price--administration'], default="")
                    room_data['deposit_key_money'] = await safe_extract_text(['span.cassetteitem_price--deposit'], default="")
                    room_data['layout'] = await safe_extract_text(['span.cassetteitem_madori'], default="")
                    room_data['area'] = await safe_extract_text(['span.cassetteitem_menseki'], default="")
                    
                    # è©³ç´°URL
                    try:
                        detail_link = await row.query_selector('a')
                        if detail_link:
                            href = await detail_link.get_attribute('href')
                            if href:
                                room_data['detail_url'] = f"{self.base_url}{href}"
                    except Exception:
                        room_data['detail_url'] = None
                    
                    # éƒ¨å±‹ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                    try:
                        room = RoomInfo(**room_data)
                        rooms_info.append(room)
                    except ValidationError as e:
                        logger.debug(f"Room validation error: {e}")
                        self.validation_errors.append({"type": "room", "data": room_data, "error": str(e)})
            
            except Exception as e:
                logger.debug(f"Error extracting room info: {e}")
            
            property_data['rooms'] = rooms_info
            
            # ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            try:
                property_obj = PropertyInfo(**property_data)
                return property_obj
            except ValidationError as e:
                logger.debug(f"Property validation error: {e}")
                self.validation_errors.append({"type": "property", "data": property_data, "error": str(e)})
                return None
                
        except Exception as e:
            logger.error(f"Unexpected error in extract_property_info: {e}")
            return None
    
    async def safe_goto_page(self, url):
        """å®‰å…¨ãªãƒšãƒ¼ã‚¸é·ç§»"""
        async def _goto():
            try:
                # User-Agentã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                await self.page.set_extra_http_headers({
                    'User-Agent': self.request_manager.get_current_user_agent(),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                })
                
                # ã‚ˆã‚Šé•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§å®‰å…¨ã«ãƒŠãƒ“ã‚²ãƒ¼ãƒˆ
                response = await self.page.goto(
                    url, 
                    wait_until="domcontentloaded",  # networkidleã‚ˆã‚Šè»½é‡
                    timeout=60000
                )
                
                if response and response.status >= 400:
                    raise Exception(f"HTTP {response.status}: {response.status_text}")
                
                return response
                
            except Exception as e:
                logger.warning(f"Navigation failed for {url}: {e}")
                raise e
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’é€šã—ã¦å®Ÿè¡Œ
        return await self.request_manager.make_request(_goto, "page")
    
    async def scrape_page(self, page_url):
        """ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆè² è·è»½æ¸›ç‰ˆï¼‰"""
        logger.info(f"Politely scraping page: {page_url}")
        
        try:
            # å®‰å…¨ãªãƒšãƒ¼ã‚¸é·ç§»
            await self.safe_goto_page(page_url)
            
            # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾Œã®è¿½åŠ å¾…æ©Ÿï¼ˆäººé–“ã‚‰ã—ã„è¡Œå‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            await asyncio.sleep(random.uniform(2.0, 4.0))
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã¨URLç¢ºèª
            title = await self.page.title()
            current_url = self.page.url
            logger.info(f"Page loaded: {title}")
            
            # ç‰©ä»¶ä¸€è¦§ã‚’å®‰å…¨ã«å–å¾—
            try:
                property_elements = await self.page.query_selector_all('div.cassetteitem')
                logger.info(f"Found {len(property_elements)} property elements")
            except Exception as e:
                logger.warning(f"Failed to find property elements: {e}")
                return []
            
            if not property_elements:
                logger.warning("No property elements found on page")
                return []
            
            properties = []
            for i, element in enumerate(property_elements):
                logger.debug(f"Processing property element {i+1}/{len(property_elements)}")
                
                # å„ç‰©ä»¶ã®å‡¦ç†é–“ã«ã‚‚å°ã•ãªå¾…æ©Ÿã‚’å…¥ã‚Œã‚‹
                if i > 0:
                    await asyncio.sleep(random.uniform(0.5, 1.5))
                
                property_info = await self.extract_property_info(element)
                if property_info:
                    properties.append(property_info)
                    
                # ç›®æ¨™ä»¶æ•°ãƒã‚§ãƒƒã‚¯
                total_rooms = sum(len(prop.rooms) for prop in self.properties + properties)
                if total_rooms >= self.target_count:
                    logger.info(f"Target count {self.target_count} reached during page processing")
                    break
            
            logger.info(f"Successfully extracted {len(properties)} properties from page")
            return properties
            
        except Exception as e:
            logger.error(f"Error scraping page {page_url}: {e}")
            return []
    
    async def get_next_page_url(self):
        """æ¬¡ãƒšãƒ¼ã‚¸URLå–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
        try:
            await asyncio.sleep(random.uniform(1.0, 2.0))  # UIæ“ä½œå‰ã®å¾…æ©Ÿ
            
            next_link = await self.page.query_selector('p.pagination-parts + p a')
            if next_link:
                href = await next_link.get_attribute('href')
                if href and href.strip():
                    next_url = f"{self.base_url}{href}"
                    logger.debug(f"Found next page: {next_url}")
                    return next_url
            
            logger.debug("No next page found")
            return None
            
        except Exception as e:
            logger.warning(f"Error getting next page URL: {e}")
            return None
    
    async def scrape_station(self, station_name):
        """å˜ä¸€é§…ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆè² è·é…æ…®ç‰ˆï¼‰"""
        self.current_station = station_name
        
        try:
            search_url = get_station_url(station_name, self.prefecture)
        except ValueError as e:
            logger.error(f"Invalid station {station_name}: {e}")
            return []
        
        logger.info(f"Starting polite scraping for station: {station_name}")
        
        current_url = search_url
        page_count = 0
        station_properties = []
        
        # é§…é–“ã®é•·ã‚ã®å¾…æ©Ÿ
        if self.current_station != station_name:
            logger.info(f"Switching to new station, waiting...")
            await asyncio.sleep(random.uniform(10.0, 20.0))
        
        while current_url and page_count < 10:  # ãƒšãƒ¼ã‚¸æ•°åˆ¶é™ã§è² è·è»½æ¸›
            try:
                page_properties = await self.scrape_page(current_url)
                valid_properties = [prop for prop in page_properties if prop is not None]
                station_properties.extend(valid_properties)
                
                logger.info(f"Scraped {len(valid_properties)} properties from {station_name} page {page_count + 1}")
                
                # çµ±è¨ˆæƒ…å ±
                station_rooms = sum(len(prop.rooms) for prop in station_properties)
                total_rooms = sum(len(prop.rooms) for prop in self.properties) + station_rooms
                logger.info(f"Station {station_name}: {station_rooms} rooms, Total: {total_rooms}")
                
                # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                if total_rooms >= self.target_count:
                    logger.info(f"Target {self.target_count} rooms reached!")
                    remaining_needed = self.target_count - sum(len(prop.rooms) for prop in self.properties)
                    station_properties = self._limit_properties(station_properties, remaining_needed)
                    break
                
                # æ¬¡ãƒšãƒ¼ã‚¸å–å¾—
                current_url = await self.get_next_page_url()
                page_count += 1
                
                # ãƒšãƒ¼ã‚¸é–“ã®å¾…æ©Ÿï¼ˆã‚ˆã‚Šé•·ãï¼‰
                if current_url:
                    wait_time = random.uniform(8.0, 15.0)
                    logger.info(f"Waiting {wait_time:.1f}s before next page...")
                    await asyncio.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"Error processing page {page_count + 1} for {station_name}: {e}")
                break
        
        # é§…åã‚’å„ç‰©ä»¶ã«è¿½åŠ 
        for prop in station_properties:
            prop.station_name = station_name
        
        logger.info(f"Completed station {station_name}: {len(station_properties)} properties")
        return station_properties
    
    def _limit_properties(self, properties, max_rooms):
        """éƒ¨å±‹æ•°åˆ¶é™ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
        limited_properties = []
        room_count = 0
        
        for prop in properties:
            if room_count + len(prop.rooms) <= max_rooms:
                limited_properties.append(prop)
                room_count += len(prop.rooms)
            else:
                needed_rooms = max_rooms - room_count
                if needed_rooms > 0:
                    prop.rooms = prop.rooms[:needed_rooms]
                    limited_properties.append(prop)
                break
        
        return limited_properties
    
    async def scrape_all_stations(self):
        """å…¨é§…ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆè² è·é…æ…®ç‰ˆï¼‰"""
        await self.create_browser()
        
        try:
            logger.info(f"Starting polite scraping session for {len(self.stations)} stations")
            
            for i, station in enumerate(self.stations):
                if sum(len(prop.rooms) for prop in self.properties) >= self.target_count:
                    logger.info("Target reached, stopping station iteration")
                    break
                
                logger.info(f"Processing station {i+1}/{len(self.stations)}: {station}")
                
                try:
                    station_properties = await self.scrape_station(station)
                    self.properties.extend(station_properties)
                    
                    total_rooms = sum(len(prop.rooms) for prop in self.properties)
                    logger.info(f"Station {station} completed. Total rooms: {total_rooms}/{self.target_count}")
                    
                    # é§…é–“ã®é•·ã‚ã®å¾…æ©Ÿ
                    if i < len(self.stations) - 1:  # æœ€å¾Œã®é§…ã§ãªã‘ã‚Œã°
                        wait_time = random.uniform(15.0, 30.0)
                        logger.info(f"Waiting {wait_time:.1f}s before next station...")
                        await asyncio.sleep(wait_time)
                        
                except Exception as e:
                    logger.error(f"Failed to scrape station {station}: {e}")
                    continue
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
            total_rooms = sum(len(prop.rooms) for prop in self.properties)
            logger.info(f"Scraping session completed:")
            logger.info(f"  Total properties: {len(self.properties)}")
            logger.info(f"  Total rooms: {total_rooms}")
            logger.info(f"  Validation errors: {len(self.validation_errors)}")
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±è¨ˆ
            self.request_manager.log_session_stats()
            
            # çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            try:
                stations_str = "-".join(self.stations)
                result = ScrapingResult(
                    properties=self.properties,
                    total_count=len(self.properties),
                    pages_scraped=len(self.stations),
                    source_url=f"Polite scraping: {stations_str}"
                )
                return result
            except ValidationError as e:
                logger.error(f"Result validation error: {e}")
                return None
                
        finally:
            await self.close_browser()
    
    def save_to_json(self, scraping_result=None, filename=None):
        """JSONä¿å­˜ï¼ˆæ—¢å­˜ã¨åŒæ§˜ï¼‰"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            stations_str = "-".join(self.stations[:3])
            if len(self.stations) > 3:
                stations_str += f"-etc{len(self.stations)}"
            filename = f'suumo_polite_{stations_str}_{timestamp}.json'
        
        os.makedirs('data', exist_ok=True)
        filepath = os.path.join('data', filename)
        
        data_to_save = scraping_result.model_dump() if scraping_result else [prop.model_dump() for prop in self.properties]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2, default=str)
        
        if self.validation_errors:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            error_filepath = os.path.join('data', f'validation_errors_polite_{stations_str}_{timestamp}.json')
            with open(error_filepath, 'w', encoding='utf-8') as f:
                json.dump(self.validation_errors, f, ensure_ascii=False, indent=2)
            logger.info(f"Validation errors saved to {error_filepath}")
        
        logger.info(f"Data saved to {filepath}")
        return filepath
    
    def save_to_csv(self, filename=None):
        """CSVä¿å­˜ï¼ˆæ—¢å­˜ã¨åŒæ§˜ï¼‰"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            stations_str = "-".join(self.stations[:3])
            if len(self.stations) > 3:
                stations_str += f"-etc{len(self.stations)}"
            filename = f'suumo_polite_{stations_str}_{timestamp}.csv'
        
        os.makedirs('data', exist_ok=True)
        filepath = os.path.join('data', filename)
        
        flattened_data = []
        for prop in self.properties:
            base_info = {
                'scraping_mode': 'polite',
                'target_stations': ', '.join(self.stations),
                'search_station': getattr(prop, 'station_name', 'unknown'),
                'building_title': prop.title,
                'address': prop.address,
                'access': prop.access,
                'building_age_area': prop.building_age_area,
                'building_age': prop.building_age,
                'station_info': ', '.join(prop.station_info),
                'min_rent': prop.min_rent,
                'max_rent': prop.max_rent,
                'scraped_at': prop.scraped_at.isoformat()
            }
            
            for room in prop.rooms:
                room_data = base_info.copy()
                room_data.update({
                    'floor': room.floor,
                    'rent': room.rent,
                    'rent_numeric': room.rent_numeric,
                    'admin_fee': room.admin_fee,
                    'admin_fee_numeric': room.admin_fee_numeric,
                    'deposit_key_money': room.deposit_key_money,
                    'layout': room.layout,
                    'area': room.area,
                    'area_numeric': room.area_numeric,
                    'detail_url': room.detail_url
                })
                flattened_data.append(room_data)
        
        df = pd.DataFrame(flattened_data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        logger.info(f"Data saved to {filepath}")
        return filepath

def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚¹ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    parser = argparse.ArgumentParser(
        description="SUUMOè³ƒè²¸ç‰©ä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·é…æ…®ç‰ˆï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ç¤¼å„€æ­£ã—ã„ãƒ¢ãƒ¼ãƒ‰ã§æ¸‹è°·é§…ã‹ã‚‰50ä»¶å–å¾—
  python polite_scraper.py --stations æ¸‹è°· --count 50 --polite
  
  # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§è¤‡æ•°é§…ã‹ã‚‰100ä»¶å–å¾—
  python polite_scraper.py --stations æ¸‹è°· æ–°å®¿ --count 100
  
  # å±±æ‰‹ç·šã‹ã‚‰å°‘é‡å–å¾—ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·æœ€å°é™ï¼‰
  python polite_scraper.py --yamanote --count 30 --polite
        """
    )
    
    parser.add_argument('--stations', nargs='+', default=['æ¸‹è°·'],
                       help='å¯¾è±¡é§…åã®ãƒªã‚¹ãƒˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ¸‹è°·)')
    parser.add_argument('--count', type=int, default=100,
                       help='å–å¾—ã™ã‚‹éƒ¨å±‹æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)')
    parser.add_argument('--prefecture', choices=['tokyo', 'kanagawa', 'saitama', 'chiba'],
                       default='tokyo', help='éƒ½é“åºœçœŒ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: tokyo)')
    parser.add_argument('--yamanote', action='store_true',
                       help='å±±æ‰‹ç·šå…¨é§…ã‚’å¯¾è±¡ã«ã™ã‚‹')
    parser.add_argument('--polite', action='store_true',
                       help='ç¤¼å„€æ­£ã—ã„ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾…æ©Ÿæ™‚é–“é•·ã‚ã€ã‚µãƒ¼ãƒãƒ¼è² è·é…æ…®ï¼‰')
    parser.add_argument('--list-stations', action='store_true',
                       help='å¯¾å¿œã—ã¦ã„ã‚‹é§…åä¸€è¦§ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†')
    parser.add_argument('--output-json', help='JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‘ã‚¹')
    parser.add_argument('--output-csv', help='CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‘ã‚¹')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º')
    
    return parser.parse_args()

async def main():
    args = parse_arguments()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    if args.list_stations:
        stations = get_supported_stations()
        print("å¯¾å¿œã—ã¦ã„ã‚‹é§…åä¸€è¦§:")
        for i, station in enumerate(stations, 1):
            yamanote_mark = " (å±±æ‰‹ç·š)" if is_yamanote_station(station) else ""
            print(f"{i:2d}. {station}{yamanote_mark}")
        return
    
    if args.yamanote:
        stations = get_yamanote_stations()
    else:
        stations = args.stations
    
    # å¯¾å¿œé§…ãƒã‚§ãƒƒã‚¯
    supported_stations = get_supported_stations()
    invalid_stations = [s for s in stations if s not in supported_stations]
    if invalid_stations:
        logger.error(f"å¯¾å¿œã—ã¦ã„ãªã„é§…å: {invalid_stations}")
        logger.error("å¯¾å¿œé§…ä¸€è¦§ã‚’ç¢ºèªã™ã‚‹ã«ã¯ --list-stations ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        return
    
    # ç¤¼å„€æ­£ã—ã„ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ä»¶æ•°ã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
    if args.polite and args.count > 200:
        logger.warning(f"ç¤¼å„€æ­£ã—ã„ãƒ¢ãƒ¼ãƒ‰ã§ã¯å¤§é‡å–å¾—({args.count}ä»¶)ã¯éæ¨å¥¨ã§ã™")
        response = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() not in ['y', 'yes']:
            logger.info("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ")
            return
    
    scraper = PoliteSuumoScraper(
        stations=stations,
        target_count=args.count,
        prefecture=args.prefecture,
        polite_mode=args.polite
    )
    
    mode_name = "ç¤¼å„€æ­£ã—ã„ãƒ¢ãƒ¼ãƒ‰" if args.polite else "é€šå¸¸ãƒ¢ãƒ¼ãƒ‰"
    logger.info(f"SUUMO Polite Scraper é–‹å§‹ ({mode_name})")
    logger.info(f"å¯¾è±¡é§…: {', '.join(stations)}")
    logger.info(f"ç›®æ¨™éƒ¨å±‹æ•°: {args.count}")
    logger.info(f"éƒ½é“åºœçœŒ: {args.prefecture}")
    
    if args.polite:
        logger.info("ğŸ“ˆ ã‚µãƒ¼ãƒãƒ¼è² è·é…æ…®ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹:")
        logger.info("  - é•·ã‚ã®å¾…æ©Ÿæ™‚é–“")
        logger.info("  - User-Agentãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³")
        logger.info("  - è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½")
        logger.info("  - ãƒªã‚¯ã‚¨ã‚¹ãƒˆç›£è¦–")
    
    try:
        result = await scraper.scrape_all_stations()
        
        if result and result.properties:
            json_path = scraper.save_to_json(result, args.output_json)
            csv_path = scraper.save_to_csv(args.output_csv)
            
            total_rooms = sum(len(prop.rooms) for prop in result.properties)
            
            logger.info("ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
            logger.info(f"ğŸ“Š çµæœçµ±è¨ˆ:")
            logger.info(f"  å¯¾è±¡é§…: {', '.join(stations)}")
            logger.info(f"  ç‰©ä»¶æ•°: {result.total_count}")
            logger.info(f"  éƒ¨å±‹æ•°: {total_rooms}")
            logger.info(f"  å‡¦ç†é§…æ•°: {result.pages_scraped}")
            if result.average_rent:
                logger.info(f"  å¹³å‡è³ƒæ–™: {result.average_rent:.0f}å††")
            logger.info(f"  é–“å–ã‚Šåˆ†å¸ƒ: {result.layout_distribution}")
            logger.info(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
            logger.info(f"  JSON: {json_path}")
            logger.info(f"  CSV: {csv_path}")
        else:
            logger.warning("ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    except KeyboardInterrupt:
        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())